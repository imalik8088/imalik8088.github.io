<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>imalik8088</title>
    <link>https://imalik8088.de/</link>
    <description>Recent content on imalik8088</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 30 Mar 2021 09:00:00 +0100</lastBuildDate><atom:link href="https://imalik8088.de/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Apache Flink Continuous Deployment</title>
      <link>https://imalik8088.de/posts/apache-flink-continuous-deployment/</link>
      <pubDate>Tue, 30 Mar 2021 09:00:00 +0100</pubDate>
      
      <guid>https://imalik8088.de/posts/apache-flink-continuous-deployment/</guid>
      <description>Coming from Kafka-Streams continuous delivery (CD) is quite an easy task, and almost no effort has to be done compared to Apache Flink. Because the state of a Kafka-Streams application is stored in Kafka, and it can build up the state after a redeployment from so-called changelog topics, therefore Kafka-Streams is also bounded to have source and sink to Apache Kafka.
Apache Flink on the other hand has the freedom to choose from a variety of source systems, e.</description>
    </item>
    
    <item>
      <title>Tired of repeated gitlab-ci files? Includes to the rescue!</title>
      <link>https://imalik8088.de/posts/gitlab-ci-include/</link>
      <pubDate>Mon, 04 Feb 2019 23:44:02 +0100</pubDate>
      
      <guid>https://imalik8088.de/posts/gitlab-ci-include/</guid>
      <description>Building pipelines aka Continuous Integration and Continuous Delivery (CI/CD) are not really new buzzwords in the tech industry or as sexy as bitcoin and friends, but I&amp;rsquo;m still quite excited about the recent release of the GitLab CE 11.7 Version which was released on 22nd January 2019. In this post we will have a look to newly added feature in GitLab CI where we will
 MAKE THE GITLAB-CI.YML DRY AGAIN</description>
    </item>
    
    <item>
      <title>ETL with Kafka</title>
      <link>https://imalik8088.de/posts/etl-with-kafka/</link>
      <pubDate>Fri, 02 Feb 2018 23:08:25 +0100</pubDate>
      
      <guid>https://imalik8088.de/posts/etl-with-kafka/</guid>
      <description>Originally published at codecentrics blog
&amp;ldquo;ETL with Kafka&amp;rdquo; is a catchy phrase that I purposely chose for this post instead of a more precise title like &amp;ldquo;Building a data pipeline with Kafka Connect&amp;rdquo;.
TLDR You don’t need to write any code for pushing data into Kafka, instead just choose your connector and start the job with your necessary configurations. And it’s absolutely Open Source!
Kafka Connect Kafka Before getting into the Kafka Connect framework, let us briefly sum up what Apache Kafka is in couple of lines.</description>
    </item>
    
    <item>
      <title>Akhlaq Malik</title>
      <link>https://imalik8088.de/about_me/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://imalik8088.de/about_me/</guid>
      <description>about_me</description>
    </item>
    
    <item>
      <title>Archive</title>
      <link>https://imalik8088.de/archives/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://imalik8088.de/archives/</guid>
      <description>archives</description>
    </item>
    
    <item>
      <title>Imprint</title>
      <link>https://imalik8088.de/imprint/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://imalik8088.de/imprint/</guid>
      <description>imprint</description>
    </item>
    
  </channel>
</rss>

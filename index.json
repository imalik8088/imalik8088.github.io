[{"content":"Coming from Kafka-Streams continuous delivery (CD) is quite an easy task, and almost no effort has to be done compared to Apache Flink. Because the state of a Kafka-Streams application is stored in Kafka, and it can build up the state after a redeployment from so-called changelog topics, therefore Kafka-Streams is also bounded to have source and sink to Apache Kafka.\nApache Flink on the other hand has the freedom to choose from a variety of source systems, e.g. Kafka, Pulsar, RabbitMQ and many other sources to consume and build the streaming application. Therefore it could not rely on changelog topics as Kafka-Streams does. Apache Flink offers out of the box following State Backends for stateful streaming processing:\n MemoryStateBackend FsStateBackend RocksDBStateBackend  For more details and an in depth description of each state backend with the limitations and the use cases, please look at the official documentation. In this article I\u0026rsquo;ll be using FsStateBackend for the examples.\nConfigure checkpointing with FsStateBackend Before starting to build up the state of a stateful Flink application you have to save the state somewhere or better to say you have to choose of one of the three available state backend implementations or you have to build your custom state backend. I\u0026rsquo;ve have configure the FsStateBackend as followed in Flink application:\nParameterTool parameter = ParameterTool.fromArgs(args); StreamExecutionEnvironment executionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment(); executionEnvironment.enableCheckpointing(Duration.ofSeconds(60).toMillis()); executionEnvironment.getCheckpointConfig().enableExternalizedCheckpoints(RETAIN_ON_CANCELLATION); executionEnvironment.setStateBackend(new FsStateBackend(parameter.get(\u0026#34;s3-uri\u0026#34;))); In order to use the FsStateBackend  you have to add the following dependency, if you are using AWS S3 for your state.\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;flink-s3-fs-hadoop\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${flink.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; Configure the application for continuous delivery After setting up the FsStateBackend for the stateful streaming application the main question is how to have a seamless, continuous delivery pipeline where your application starts with the latest checkpoint from the old deployed version (job id). To my surprise, I didn\u0026rsquo;t find a lot of resources out there in the internet so I looked around and also asked the community. I got the following answer and the following tools that could be helpful for a seamless deployment of Flink applications.\n \u0026hellip; but the deployment process involves several steps, any of which might potentially fail. So it\u0026rsquo;s a bit complex to fully automate.\n  Ververica Platform Community Edition - build by the creator of Flink Flink-Deployer - a tool build by ing-diba bank  The mentioned tools would be a bigger time invest and it would need to be investigated more before using these tools for a project in a bigger corporation. So we have to came up with something else in order to make a seamless deployment work in Kubernetes, where all our Apache Flink applications are running in.\nAfter Bundling the application in an official Flink docker container, the application can be started with following arguments passed to a Kubernetes deployment:\ncontainers: - name: jobmanager image: your_awesome_flink_app:\u0026lt;TAG\u0026gt; args: - \u0026#34;standalone-job\u0026#34; - \u0026#34;--job-classname\u0026#34; - \u0026#34;a.b.c.MainClassOfStreamingApp\u0026#34; - \u0026#34;--fromSavepoint\u0026#34; - \u0026#34;s3://bucket_name/dir_were_to_store_the_state_v1/last-state/_metadata\u0026#34; - \u0026#34;--s3-uri\u0026#34; - \u0026#34;s3://bucket_name/dir_were_to_store_the_state_v1\u0026#34; - \u0026#34;-Ds3.access-key=$(AWS_ACCESS_KEY_ID)\u0026#34; - \u0026#34;-Ds3.secret-key=$(AWS_SECRET_KEY)\u0026#34; The important part is the --fromSavepoint argument and the last-state directory in S3 which is being created or better to say copied over by a Kubernetes cronjob that runs every minute. The following bash script for the cronjob is the heart, with whom we are able to have a continuous delivery without changing the path of the --fromSavepoint argument to the old job id which is being randomly created for each jobmanger.\n#!/usr/bin/env sh  APP_NAME=\u0026#34;$1\u0026#34; JOBMANAGER_URL=\u0026#34;$2\u0026#34; shift; if [ \u0026#34;${APP_NAME}\u0026#34; == \u0026#34;--help\u0026#34; -o \u0026#34;${APP_NAME}\u0026#34; == \u0026#34;\u0026#34; -o \u0026#34;${JOBMANAGER_URL}\u0026#34; == \u0026#34;\u0026#34; ]; then echo \u0026#34;Usage: \u0026lt;app-name\u0026gt; \u0026lt;jobmanager-url\u0026gt; app-name: subfolder of the bucket where the state needs to be saved. This dir needs to be created manually at first jobmanager-url: k8s service url of the jobmanager without port \u0026#34; exit 1 else set -e # (1) looking for the job id RUNNING_JOBS=$(curl -s $JOBMANAGER_URL:8081/jobs) JOBID=$(echo $RUNNING_JOBS | jq -r \u0026#39;.jobs | .[0] | select(.status==\u0026#34;RUNNING\u0026#34;) | .id\u0026#39;) if [ -z \u0026#34;$JOBID\u0026#34; ]; then echo \u0026#34;No Flink job id found\u0026#34; exit 1 fi echo \u0026#34;Looking for $JOBIDjob id state\u0026#34; # (2) link current checkpoint to last_state LAST_METADATA=$(aws s3 ls s3://$AWS_BUCKET_NAME/$APP_NAME/$JOBID --recursive | sort | grep _metadata | grep -v \u0026#34;last-state\u0026#34; | tail -n1 | awk \u0026#39;{print $4}\u0026#39;) echo $LAST_METADATA # (3) timeout -t 60 aws s3 cp s3://$AWS_BUCKET_NAME/$LAST_METADATA s3://$AWS_BUCKET_NAME/$APP_NAME/last-state/_metadata exit $? fi exec \u0026#34;$@\u0026#34; (1) Apache Flink\u0026rsquo;s JobManager API is being used to get all running jobs and select the first (limitation of this script) jobs id.\n(2) This Job id is being used to grep the _metadata file by from the latest checkpoint of the currently running Flink job.\nThe last step (3) is copying the found metadata file and to the last-state directory from where the application is restoring from during restart or reployment.\nThe shown script is bundled in a container as a docker-entrypoint file where aws cli, curl and jq should be available. This container is then being used and started as Kubernetes cronjob with the following arguments:\ncontainers: - name: aws-container image: image-with-aws-jq-and-curl-and-the-script:latest args: - \u0026#34;dir_were_to_store_the_state_v1\u0026#34; - \u0026#34;k8s-service-of-flink-jobmanager\u0026#34; Conclusion In this blog post the challenges of a stateful streaming application in Apache Flink for a continuous delivery process has been dicussed and a solution that worked for my client has been shown. I\u0026rsquo;m quite interested to get feedback by the Flink community, in which pitfalls this approach may lead into and in which scenarios it may not work.\n","permalink":"https://imalik8088.de/posts/apache-flink-continuous-deployment/","summary":"Coming from Kafka-Streams continuous delivery (CD) is quite an easy task, and almost no effort has to be done compared to Apache Flink. Because the state of a Kafka-Streams application is stored in Kafka, and it can build up the state after a redeployment from so-called changelog topics, therefore Kafka-Streams is also bounded to have source and sink to Apache Kafka.\nApache Flink on the other hand has the freedom to choose from a variety of source systems, e.","title":"Apache Flink Continuous Deployment"},{"content":"Building pipelines aka Continuous Integration and Continuous Delivery (CI/CD) are not really new buzzwords in the tech industry or as sexy as bitcoin and friends, but I\u0026rsquo;m still quite excited about the recent release of the GitLab CE 11.7 Version which was released on 22nd January 2019. In this post we will have a look to newly added feature in GitLab CI where we will\n MAKE THE GITLAB-CI.YML DRY AGAIN\n TL;DR As of GitLab CE version 11.7, you can use include commands to include your tasks from other .yml files. This is a big deal, because you don‘t have to repeat your self in each of your projects any more. Especially in projects that consist of a large number of microservices sharing (parts) of the pipeline definition is major win.\nGitLab Even after the acquisition of Github by Microsoft, I still am a big fan and daily visitor of Github to look for new things around in the tech projects of my interest.\nHowever, since approx. 1.5 years I‘m often confronted with GitLab installations at work and customers to serve as the internally hosted git service and I‘m kind of getting more and more to know about the strength of GitLab and the ecosystem around that, which is actually quite impressive. GitLab helps you as a developer to be more productive where it avoiding switches between for example Jenkins for your pipeline and code repository in Gogs.\nIntroduction Writing pipelines in Gitlab CI is quite an easy task when compared to writing Jenkins pipelines, at least for me. To be honest, I\u0026rsquo;m still trying to hide my Jenkins stuff that is related to private projects. On the other hand, GitLab CI is quite easy to understand and has great documentation where you can start right away without learning any special programming language (e.g. Groovy).\nUsage of Include Tag So the central part of this post is the newly available include tags{:target=\u0026quot;_blank\u0026quot;} feature in the Gitlab CE 11.7 version. Using them, you have the possibility to include other CI definitions, therefore allowing you to re-use your jobs from other files outside of your main repository. It is also possible to include remote .yaml files from other organizations. In the following, I will present some practical examples and some personal experience with this new feature.\nIn the following sections we will build the following pipeline:\n Demo project\nShared repo with pipeline job definitions\n 1. Include snippets from the same repo to build your pipeline The simplest way to extract your pipeline jobs from the main .gitlab-ci.yml is to move out the job definitions to a separate directory inside the same repo. In the following example, I moved the job definitions to the ci directory.\n. ├── .gitlab-ci.yml ├── ci │ └── local-job.yml The local-job.yml file echos some variable and also demonstrates the functionality to override job definitions that have already been defined in the main gitlab-ci.yml file, here the script and variables keys.\ninclude: - local: \u0026#39;/ci/local-job.yml\u0026#39; variables: OVERRIDE_VAR: \u0026#34;OVERRIDE VALUE IN CI FILE\u0026#34; VAR1: \u0026#34;OVERRIDE VAR 1 FROM CI FILE\u0026#34; DEBUG: \u0026#39;TRUE\u0026#39; local-job: script: - echo \u0026#34;OVERRIDE FROM SCRIPT BLOCK OF MAIN GITLAB CI\u0026#34; - echo $OVERRIDE_VAR  # --- OUTPUT OF THE JOB # $ echo \u0026#34;before_script from \u0026#39;include-local\u0026#39; task\u0026#34; # before_script from \u0026#39;include-local\u0026#39; task  # $ echo \u0026#34;OVERRIDE FROM SCRIPT BLOCK OF MAIN GITLAB CI\u0026#34; # OVERRIDE FROM SCRIPT BLOCK OF MAIN GITLAB CI  # $ echo $OVERRIDE_VAR # OVERRIDE VALUE IN CI FILE 2. Include a job definition from another repository within the same organization Having some parts of your build in a separate file can help clean up big pipeline definitions, but especially with multiple repositories this is still too much repetition. For this use-case, GitLab also allows you to include from a another repository in the same organization:\ninclude: - project: \u0026#39;imalik8088/gitlab-ci-pipeline\u0026#39; file: \u0026#39;/maven-package.yml\u0026#39; maven-package: artifacts: paths: - target/gitlab-ci-1.0-SNAPSHOT.jar  This includes a file called maven-package.yml from the same GitLab organization but a different repository. Here again, you are able to change the default behavior by overriding the blocks or variables. This job definition will expand the original job definition, where we are defining a project-specific artifact where a .jar by the java application is been used to define it as an artifact in the context of GitLab. This comes in quite handy when you don\u0026rsquo;t want any defaults but project-specific definition.\n3. Include a job definition from a URL The third and last option we will look at, is to include a job definition file via an arbitrary URL. This is even more powerful than the previous options, because you are free to query any http endpoint that returns the file. Here is an example of that:\ninclude: - remote: \u0026#39;https://gitlab.com/imalik8088/gitlab-ci-pipeline/raw/master/build-docker.yml\u0026#39; build-docker: variables: IMAGE_NAME: gitlab-ci-java dependencies: - maven-package In this job definition, we are building a docker image. You have to define usually some extra code if you want to use docker in your CI (better known as Docker-in-Docker). This ceremony is now hidden inside an external job definition. The main steps can still be overridden which leads to more concise pipelines where the boilerplate can be hidden behind the scenes. In this example, we are depending on the maven-package job where the artifact is transferred to this job definition and is used to build the docker image including the compile, test .jar from a previous stage.\nNotes  One thing to note here is, that although you are defining your stages in the remote job definition you have to list all of the stage in the main .gitlab-ci file which defines the order in which jobs will be executed. I have included in the demo project a gitlab ci file from a personal project were its about 125 lines of code and lot of repetitions. We can imagine how this gitlab ci file can benefit from the new feature described in this post and almost every organization have repetitive jobs like build, test, coverage, build docker images, deploy, etc.  Conclusion Gitlab and Gitlab CI have been build in the era of cloud-native applications and the whole integration of GitLab and CI/CD makes the whole DevOps lifecycle a breeze. With the newly added feature to include jobs definition from a local definition, a central repository or remote URL, we can finally get rid of all the repetition in our build definitions.\nThanks for the review Markus!\n","permalink":"https://imalik8088.de/posts/gitlab-ci-include/","summary":"Building pipelines aka Continuous Integration and Continuous Delivery (CI/CD) are not really new buzzwords in the tech industry or as sexy as bitcoin and friends, but I\u0026rsquo;m still quite excited about the recent release of the GitLab CE 11.7 Version which was released on 22nd January 2019. In this post we will have a look to newly added feature in GitLab CI where we will\n MAKE THE GITLAB-CI.YML DRY AGAIN","title":"Tired of repeated gitlab-ci files? Includes to the rescue!"},{"content":"Originally published at codecentrics blog\n\u0026ldquo;ETL with Kafka\u0026rdquo; is a catchy phrase that I purposely chose for this post instead of a more precise title like \u0026ldquo;Building a data pipeline with Kafka Connect\u0026rdquo;.\nTLDR You don’t need to write any code for pushing data into Kafka, instead just choose your connector and start the job with your necessary configurations. And it’s absolutely Open Source!\nKafka Connect Kafka Before getting into the Kafka Connect framework, let us briefly sum up what Apache Kafka is in couple of lines. Apache Kafka was built at LinkedIn to meet the requirements that message brokers already existing in the market did not meet – requirements such as scalable, distributed, resilient with low latency and high throughput. Currently, i.e. 2018, LinkedIn is processing about 1.8 petabytes of data per day through Kafka. Kafka offers a programmable interface (API) for a lot of languages to produce and consume data.\nKafka Connect Kafka Connect has been built into Apache Kafka since version 0.9 (11/2015), although the idea had been in existence before this release, but as a project named Copycat. Kafka Connect is basically a framework around Kafka to get data from different sources in and out of Kafka (sinks) into other systems e.g. Cassandra with automatic offset management, where as a user of the connector you don’t need to worry about this, but rely on the developer of the connector.\nBesides that, in discussions I have often come across people who were thinking that Kafka Connect was part of the Confluent Enterprise and not a part of Open Source Kafka. To my surprise, I have even heard it from a long-term Kafka developer. That confusion might be due to the fact that if you google the term Kafka Connect, the first few pages on Google are by Confluent and the list of certified connectors.\nKafka Connect has basically three main components that need to be understood for a deeper understanding of the framework.\nConnectors are, in a way, the “brain” that determine how many tasks will run with the configurations and how the work is divided between these tasks. For example, the JDBC connector can decide to parallelize the process to consume data from a database (see figure 2).\nTasks contain the main logic of getting the data into Kafka from external systems by connecting e.g. to a database (Source Task) or consuming data from Kafka and pushing it to external systems (Sink Task).\nWorkers are the part that abstracts away from the connectors and tasks in order to provide a REST API (main interaction), reliability, high availability, scaling, and load balancing.\nStandalone Kafka connect can be started in two different modes. The first mode is called standalone and should be used only in development because offsets are being maintained on the file system. This would be really bad if you were running this mode in production and your machine was unavialable. This could cause the loss of the state, which means the offset is lost and you as a develeoper don’t know how much data has been processed.\n# connnect-standalone.properties offset.storage.file.filename=/tmp/connect.offsets Distributed The second mode is called distributed. There, the configuration, state and status are stored in Kafka itself in different topics which benefit from all Kafka characteristics such as resilience and scalability. Workers can start on different machines and the group.id attribute in the .properties file will eventually form the Kafka Connect Cluster which can be scaled up or down.\n# connnect-distributed.properties group.id=connect-cluster config.storage.topic=connect-configs offset.storage.topic=connect-offsets status.storage.topic=connect-status So let’s look in the content of the pretty self-explanatory topic use in the configuration file:\n// TOPIC =\u0026gt; connect-configs {\u0026#34;properties\u0026#34;: { \u0026#34;connector.class\u0026#34;:\u0026#34;c.e.t.k.c.twitter.TwitterSourceConnector\u0026#34;, \u0026#34;twitter.token\u0026#34;:\u0026#34;XXXX\u0026#34;, \u0026#34;tasks.max\u0026#34;:\u0026#34;1\u0026#34;, \u0026#34;track.terms\u0026#34;:\u0026#34;frankfurt\u0026#34;, \u0026#34;task.class\u0026#34;:\u0026#34;c.e.t.k.c.twitter.TwitterSourceTask\u0026#34;, \u0026#34;twitter.secret\u0026#34;:\u0026#34;XXX\u0026#34;, \u0026#34;name\u0026#34;:\u0026#34;twitter-source\u0026#34;, \u0026#34;topic\u0026#34;: \u0026#34;twitter\u0026#34;, \u0026#34;twitter.consumersecret\u0026#34;:\u0026#34;XXXXXX\u0026#34;, \u0026#34;twitter.consumerkey\u0026#34;:\u0026#34;XXXXX\u0026#34; }} // response {\u0026#34;tasks\u0026#34;:1} {\u0026#34;state\u0026#34;:\u0026#34;STARTED\u0026#34;} // TOPIC =\u0026gt; connect-offsets {\u0026#34;tweetId\u0026#34;:968476484095610880} {\u0026#34;tweetId\u0026#34;:968476527108263936} // TOPIC =\u0026gt; connect-status {\u0026#34;state\u0026#34;:\u0026#34;RUNNING\u0026#34;,\u0026#34;trace\u0026#34;:null,\u0026#34;worker_id\u0026#34;:\u0026#34;connect:8083\u0026#34;, \u0026#34;generation\u0026#34;:2} {\u0026#34;state\u0026#34;:\u0026#34;UNASSIGNED\u0026#34;,\u0026#34;trace\u0026#34;:null,\u0026#34;worker_id\u0026#34;:\u0026#34;connect:8083\u0026#34;, \u0026#34;generation\u0026#34;:2} {\u0026#34;state\u0026#34;:\u0026#34;RUNNING\u0026#34;,\u0026#34;trace\u0026#34;:null,\u0026#34;worker_id\u0026#34;:\u0026#34;connect:8083\u0026#34;, \u0026#34;generation\u0026#34;:3} The output shown here of the messages are just the values, the key of the message is used to identify the different connectors.\nInteraction pattern There is also a different interaction pattern normally between the standalone and distributed mode – in a non-production environment where you just want to test out a connector, for example, and you want to set manually the offset of your choice. You can start the standalone mode with passing in the sink or source connector that you want to use, e.g. bin/kafka-connect config/connect-standalone.properties config/connect-file-source.properties config/other-connector.properties.\nOn the other hand, you can start the Kafka Connect worker in the distributed mode with the following command: bin/kafka-connect config/connect-distributed.properties. After that, you can list all available connectors, start, change configurations on the fly, restart, pause and remove connectors via the exposed REST API of the framework. A full list of supported endpoints can be found in the offical Kafka Connect documentation.\nExample So let’s have a closer look at an example of a running data pipeline where we are getting some real time data from Twitter and using the kafka-console-consumer to consume and inspect the data.\n\nHere is the complete example shown in the terminal recording: Github repository. You can download and play around with the example project.\nConclusion In this blog post, we covered the high-level components that are the building blocks of the Kafka Connect framework. The latter is a part of the Apache Kafka Open Source version that allows data engineers or business departments to move data from one system to another without writing any code via Apache Kafka’s great characteristics, of which we barely scratched the surface in this post. So happy connecting…\n","permalink":"https://imalik8088.de/posts/etl-with-kafka/","summary":"Originally published at codecentrics blog\n\u0026ldquo;ETL with Kafka\u0026rdquo; is a catchy phrase that I purposely chose for this post instead of a more precise title like \u0026ldquo;Building a data pipeline with Kafka Connect\u0026rdquo;.\nTLDR You don’t need to write any code for pushing data into Kafka, instead just choose your connector and start the job with your necessary configurations. And it’s absolutely Open Source!\nKafka Connect Kafka Before getting into the Kafka Connect framework, let us briefly sum up what Apache Kafka is in couple of lines.","title":"ETL with Kafka"}]